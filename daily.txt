10/18/20
1. import re
1.1 exceptions: what kind of exceptions and how to handle
1.2 Python assert
2. itertool
3. json files ops
4. glob
5. logging
6. from pywinauto.application import Application
7. panda
8. IOError, OSError
9. parse large json file
11.simplejson
12. jsonpath
13. generator
14. functools
# Python Name convention
# Variable/function names are lower_caseandseparated_with_underscores
# Named constants are in ALL_CAPITAL_LETTERS
# Classes are in CamelCase
# topological sort
# bucket sort
# Block bucket: 307. Range Sum Query - Mutable
# all, any, sum

# itertools
# itertools.izip_longest
# functools.cmp_to_key(func)

11/17/20
pytest raise
pytest monkeypatch

4/1/21
1. conversion between csv and json
2. ijson, Pandas
3. how to deal with large json file
4. Using linux command: jq, split, sed ,awk
jq, in its own words, is "a lightweight and flexible command-line JSON processor.
You can use it to slice and filter and map and transform structured data
with the same ease that sed, awk, grep.
5. pickle
6. python filter, any, all
7. dict comprehension
output_dict = [{k:v for k,v in x.items() if k in ["Item", "Price"]} for x in input_dict]
8. -----

Windows Installation
convert json to csv files, then parse csv file line by line
The issue here is that JSON, as a format, is generally parsed in full and then handled in-memory,
which for such a large amount of data is clearly problematic.
The solution to this is to work with the data as a stream - reading part of the file, working with it, and then repeating.
The best option appears to be using something like ijson - a module that will work with JSON as a stream, rather than as a block file
https://www.dataquest.io/blog/python-json-tutorial/
https://blog.softhints.com/python-read-huge-json-file-pandas/
https://blog.softhints.com/python-convert-json-to-json-lines/
____
how to create large json file python
tools to process json data
_____
https://lpetr.org/author/lpetrazickis/page/2/
https://www.aylakhan.tech/?p=27
python memory profiler
https://sodocumentation.net/python/topic/2475/-args-and---kwargs
https://www.dataquest.io/blog/python-api-tutorial/
++++++++++++++++

9. pandas vs csv

pandas is for processing data; pandas has more power than python csv module
pandas will export processed data to be saved in csv

10. logging module
python logging level to set

11. tqdm
Instantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable)
from tqdm import tqdm
for i in tqdm(range(100000000)):
    i = 1
12. Requests-Cache
13. time
# time process

from datetime import datetime
import time

now = datetime.now()
print(now.strftime("%Y-%m-%d, %H:%M:%S"))
14. pip install requests-cache
import requests
import requests_cache

url = "http://jasonrigden.com"
requests_cache.install_cache("my_sqlite_db", expire_after=1000) # expire in 1000 sec
# requests_cache.disabled() # disable cache
# requests_cache.clear() # clear cache
# requests_cache.uninstall_cache() # remove cache

r = requests.get(url)
print(r.from_cache) # First time False
# 2nd time run, return True

5/4/21
# Virtual Machines
